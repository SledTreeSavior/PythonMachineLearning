# -*- coding: utf-8 -*-
"""hw6-posted.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZpRDeRA1fxGMqGvLFEl5IOWYp43CgwlI

# COMP4220: Machine Learning, Fall 2021, Assignment 6

# **Due: ** 

> ## **Please submit one pdf file for all questions.**

# 1. List five hyperparameters you can tweak in a basic neural network?

> Number of hidden layers, Number of neurons per hidden layer, Activation functions used in the hidden & output layers, Weight initialization, and Learning rate are five examples of hyperparameters in a basic neural network.

# 2. What is backpropagation and how does it work?

> Backpropagation is a training algorithm which is a modified version of gradient descent that computes gradients automatically. It does this by passing once through the dataset forward, and once backwards. In doing so, the algorithm is left with a set of gradient's of error. The algorithm then performs gradient descent on the set of gradients. The algorithm repeats these steps until a solution is reached.

# 3. How many kind of padding are there and what are their differences?

> There are two different kinds of padding, SAME and VALID padding. VALID padding does not use padding and only drops the right-most columns. SAME padding uses "zero padding", trying to pad evently left and right with '0's, but always padding an extra '0' on the right if the number added is odd.

# Programming Assignment (Artificial Neural Network-ANN)
"""

# Importing libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.compose import ColumnTransformer
import keras
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, f1_score, recall_score
import matplotlib.pyplot as plt
tf.__version__

# Importing the dataset. This dataset describes churning, which is
# the rate at which customers stop doing business with a company
from google.colab import drive
drive.mount('/content/drive')
dataset = pd.read_csv("/content/drive/My Drive/School/Machine Learning/Homework/HW6/Churn_Modelling.csv")
print(dataset)

"""## 1. Looking at the dataset we can see that the first 3 columns are not essential for our model. 
> Make a X variable that contains all other columns except the first three columns and Exited (label) <br/>
> Make a Y variable (the Exited column) <br/>
"""

X = dataset.iloc[:, 3:13].values # Get rid of useless columns # start "CreditScore" column
y = dataset.iloc[:, 13].values   # The last column "Exited" is our dependent variable

"""## 2. In X there are Geography and Gender columns that are in string format which we can't use for training. Thus we should transform it into numerical type to train our model.
> Use LabelEncoder and OneHotEncoder from sklearn.preprocessing to <br/>
> transform the "Geography" and "Gender" columns into numberical data type
"""

# Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X_1 = LabelEncoder()
X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])
labelencoder_X_2 = LabelEncoder()
X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])
ct = ColumnTransformer([("Geogrophy", OneHotEncoder(), [1])], remainder = 'passthrough')
X = ct.fit_transform(X)
X = X[:, 1:]

"""## 3. Split the dataset into the Training set and Test set (test_size = 0.2)"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""## 4. Apply Feature Scaling to all features before training a neural network"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_test = scaler.fit_transform(X_test)
X_train = scaler.fit_transform(X_train)

"""## 5. Let's build ANN model by using the Keras sequential package
> Initalize the sequential model <br/>
> Add the input layer and the first hidden layer <br/>
> Hint: For the input layer use (units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11)
"""

import keras
from keras.models import Sequential
from keras.layers import Dense

classifier = Sequential()

classifier.add(keras.layers.Input(shape=X_train.shape[1:]))

classifier.add(keras.layers.Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))

"""## 6. Add the second hidden layer
> Hint:(units = 6, kernel_initializer = 'uniform', activation = 'relu') 
"""

classifier.add(keras.layers.Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))

"""## 7. Add the output layer
> Hint: (units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')

"""

classifier.add(keras.layers.Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))

"""## 8. Compile the ANN
> hint: (optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']))
"""

classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

"""## 9. Fit the ANN to the training set
> (batch_size = 5, epochs = 20)
"""

batch_size = 5
epochs = 20
history = classifier.fit(X_train, y_train, batch_size, epochs)

"""## 10. Make predictions and evaluate the model
> hint: just consider y_pred the values where y_pred is greater than 0.5 <br/>
> (y_pred = (y_pred > 0.5)) <br/>
> Make the confusion matrix and show the result <br/>
> Evalue the precision, accuracy, recall, and f1 score and show the result <br/>
"""

y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)

from sklearn.metrics import confusion_matrix
cnf_matrix_svm = confusion_matrix(y_test, y_pred)
cnf_matrix_svm

from sklearn.metrics import precision_score
print(precision_score(y_test, y_pred, average=None))

print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))

from sklearn.metrics import recall_score
print(recall_score(y_test, y_pred))

from sklearn.metrics import f1_score
print(f1_score(y_test, y_pred))

"""## 11. Compute the accuracy, precision, recall, and f1 score"""

print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))
print('Precision: {}'.format(precision_score(y_test, y_pred)))
print('Recall: {}'.format(recall_score(y_test, y_pred)))
print('F1 Score: {}'.format(f1_score(y_test, y_pred)))

"""## 12. Why is the recall score so low? Name a way to improve this.

> It looks like we have quite a few false negatives, 102 in fact. Finding a way to reduce these false negatives would help to improve the recall score because recall is calculated inversely with false negatives. Furthermore, the precision score is quite a bit higher than the recall score. Prioritizing recall more would lower precision, but improve recall. To improve recall, one could look at the confusion matrix to try to understand why and how data is being classified incorrectly and in the wrong class. Increasing the number of samples used, or fixing labelling errors would improve recall.
"""