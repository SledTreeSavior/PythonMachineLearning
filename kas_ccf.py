# -*- coding: utf-8 -*-
"""KAS_CCF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FxKWopjWR_z0w3rEJ4dHz956t7K2cxSo
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import plot_confusion_matrix
from sklearn.ensemble import BaggingClassifier
import seaborn as sns
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import plot_precision_recall_curve
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.tree import DecisionTreeClassifier
import tensorflow as tf
from sklearn.compose import ColumnTransformer
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import classification_report
from matplotlib.pyplot import figure
from sklearn import metrics

# import the dataset
from google.colab import drive
drive.mount('/content/drive')
dataset = pd.read_csv("/content/drive/My Drive/School/Machine Learning/Final/creditcard.csv")
print(dataset)

# install imbalanced-learn
!pip install imbalanced-learn

# create X and y
X = dataset.drop(columns = ['Class'])
y = dataset['Class']

# train test split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)
print(X_train.shape)
print(X_test.shape)

# show unique elements
# this shows the class imbalance
unique_elements, counts_elements = np.unique(y_test, return_counts=True)
print(unique_elements, counts_elements)

# scale the data
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""# For classification I will use 3 of the following algorithms:
> logistic regression, support vector machines, decision trees, random forest, and neural network models
# For classification I will use the following metrics:
> precision, recall, and ROC curves

# First I will use Logisitc Regression:

# I am going to attempt to rebalance the data and visualize the effect it has on important metrics like precision, recall, etc before I choose the best method of rebalancing.
> It is important that I show the rebalance, but this will be the same for all of my rebalancing algorithms. I will also show the confusion matrix and some important metrics after creating a new logistic regression algorithm so I may compare their performance.
"""

#visualize the imbalance of the data in the base set
#the data is incredibly imbalanced towards non-fraudulent transactions
plt.rcParams["figure.figsize"] = (6,6)
unique_elements, counts_elements = np.unique(y, return_counts=True)
fig1, ax1 = plt.subplots()

ax1.pie(counts_elements, labels=unique_elements, autopct='%1.1f%%',
        shadow=True, startangle=90, textprops={'fontsize': 18})

plt.show()

# default logistic regression
log_reg = LogisticRegression(max_iter=1000).fit(X_train, y_train)
y_pred = log_reg.predict(X_test)

#display the confusion matrix
sns.set_context("poster")
disp = plot_confusion_matrix(log_reg, X_test, y_test,
                    cmap = 'cividis', colorbar=False)

print("Accuracy: ", accuracy_score(y_test, y_pred))
print("Precision: ", precision_score(y_test, y_pred))
print("Recall: ", recall_score(y_test, y_pred))
print("Classifier Score: ", log_reg.score(X_test, y_test))

"""> In the above model, accuracy and precision are strong, but recall is low. Recall is the most important metric here, because it shows times when the algorithm missed a fraudulent transaction. Precision simply shows how rare false positives are, but in real life examples, false positives are quite often. This is because they are negligible compared to a missed fraudulent transaction. We will optimize recall because of this.

> Here I changed the class_weight to "balanced". This makes the precision much worse (more false positives), but actual fraudulent transactions are caught at a much higher rate. This is very important, so the algorithm has already improved.
"""

# log_reg_b = LogisticRegression(max_iter = 1000, class_weight="balanced").fit(X_train, y_train)
# y_pred_b = log_reg_b.predict(X_test)

# sns.set_context("poster")

# disp = plot_confusion_matrix(log_reg_b, X_test, y_test,
#                     cmap = 'cividis', colorbar=False)

# print("Accuracy: ", accuracy_score(y_test, y_pred_b))
# print("Precision: ", precision_score(y_test, y_pred_b))
# print("Recall: ", recall_score(y_test, y_pred_b))
# print("Classifier Score: ", log_reg.score(X_test, y_test))

"""> Now I'll try to actually balance the data. I'll start with a random over sampler. This will over sample the minority class. Since we know that using class_weight="balanced" provides better results I'll use that while creating my new algorithm."""

ros = RandomOverSampler(random_state=0)
X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)

plt.rcParams["figure.figsize"] = (6,6)
unique_elements, counts_elements = np.unique(y_train_ros, return_counts=True)
fig1, ax1 = plt.subplots()

ax1.pie(counts_elements, labels=unique_elements, autopct='%1.1f%%',
        shadow=True, startangle=90, textprops={'fontsize': 18})

plt.show()

log_reg_ros = LogisticRegression(max_iter = 1000, class_weight="balanced").fit(X_train_ros, y_train_ros)
y_pred_ros = log_reg_ros.predict(X_test)

sns.set_context("poster")

disp = plot_confusion_matrix(log_reg_ros, X_test, y_test,
                    cmap = 'cividis', colorbar=False)

print("Accuracy: ", accuracy_score(y_test, y_pred_ros))
print("Precision: ", precision_score(y_test, y_pred_ros))
print("Recall: ", recall_score(y_test, y_pred_ros))
print("Classifier Score: ", log_reg_ros.score(X_test, y_test))

"""> Though our metric scores have overall lowered, our algorithm is actually performing better than the other iterations. We have successfully lowered our false positives by a small amount.

> Now I'll try another method. I'll use smote to resample the dataset.
"""

# smote = SMOTE()
# X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# plt.rcParams["figure.figsize"] = (6,6)
# unique_elements, counts_elements = np.unique(y_train_smote, return_counts=True)
# fig1, ax1 = plt.subplots()

# ax1.pie(counts_elements, labels=unique_elements, autopct='%1.1f%%',
#         shadow=True, startangle=90, textprops={'fontsize': 18})

# plt.show()

# log_reg_smote = LogisticRegression(max_iter = 1000).fit(X_train_smote, y_train_smote)
# y_pred_smote = log_reg_smote.predict(X_test)

# sns.set_context("poster")

# disp = plot_confusion_matrix(log_reg_smote, X_test, y_test,
#                     cmap = 'cividis', colorbar=False)

# print("Accuracy: ", accuracy_score(y_test, y_pred_smote))
# print("Precision: ", precision_score(y_test, y_pred_smote))
# print("Recall: ", recall_score(y_test, y_pred_smote))
# print("Classifier Score: ", log_reg_smote.score(X_test, y_test))

"""> Using smote, I was able to lower the false negatives by 1, but it raised the false positives quite a bit. Because of this, I will prefer the rebalancing using random over sampling, but let's keep trying new methods.

> I'm going to try using the random under sampler now. This will under sample the majority dataset.
"""

# rus = RandomUnderSampler(random_state=0)
# X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)

# plt.rcParams["figure.figsize"] = (6,6)
# unique_elements, counts_elements = np.unique(y_train_rus, return_counts=True)
# fig1, ax1 = plt.subplots()

# ax1.pie(counts_elements, labels=unique_elements, autopct='%1.1f%%',
#         shadow=True, startangle=90, textprops={'fontsize': 18})

# plt.show()

# log_reg_rus = LogisticRegression(max_iter = 1000).fit(X_train_rus, y_train_rus)
# y_pred_rus = log_reg_rus.predict(X_test)

# sns.set_context("poster")

# disp = plot_confusion_matrix(log_reg_rus, X_test, y_test,
#                     cmap = 'cividis', colorbar=False)

# print("Accuracy: ", accuracy_score(y_test, y_pred_rus))
# print("Precision: ", precision_score(y_test, y_pred_rus))
# print("Recall: ", recall_score(y_test, y_pred_rus))
# print("Classifier Score: ", log_reg_rus.score(X_test, y_test))

"""> It seems like the random under sampler provides the worst rebalaning of all algorithms so far (aside from the first). From the random over sampler, it simply increased the number of false positives. We won't be using this algorithm.

> Now I'll try using a bagging classifier with my best performing algorithm. That would be the random over sampler.
"""

# here you can see that im using bagging classifier, but im setting the base estimator to Logistic Regression using the resampling from random over sampling.
# also make sure to set the class weight to "balanced" again here, since we proved that it helped
# log_reg_bc = BaggingClassifier(base_estimator=LogisticRegression(max_iter=1000, class_weight="balanced"),
#                               n_estimators=10,
#                               random_state=0).fit(X_train_ros, y_train_ros)
# y_pred_bc = log_reg_bc.predict(X_test)

# sns.set_context("poster")

# disp = plot_confusion_matrix(log_reg_bc, X_test, y_test,
#                     cmap = 'cividis', colorbar=False)

# print("Accuracy: ", accuracy_score(y_test, y_pred_bc))
# print("Precision: ", precision_score(y_test, y_pred_bc))
# print("Recall: ", recall_score(y_test, y_pred_bc))
# print("Classifier Score: ", log_reg_bc.score(X_test, y_test))

"""> It seems that the bagging classifier provides decent resutls, but the false positive rate is still higher than simple random over sampling. Because of this, our best resampled data will be X_train_ros and y_train_ros and our best Logistic Regression algorithm will be log_reg_ros.

> I'm going to set the values from the random over sampler to be more simple, since we are going to use these for the rest of the project.
"""

# simplifying vars
#X_train = X_train_ros
#y_train = y_train_ros
#log_reg = log_reg_ros
#y_pred = y_pred_ros

# creating classification report
log_reg_report = classification_report(y_test, y_pred_ros)
print(log_reg_report)

"""> As you can see in the classification report, our recall is very high for both the majority and minority class!

> Now I'll use cross val predict to train the model on this data
"""

# note that y_scores could be higher if I useed the unbalanced dataset, but it would perform worse in a real world scenario
y_train_pred_log_reg = cross_val_predict(log_reg_ros, X_train_ros, y_train_ros, cv = 3)
y_scores_log_reg = cross_val_score(log_reg_ros, X_train_ros, y_train_ros, cv = 3)
print(y_scores_log_reg)

# i'll have to train y_scores with cross val predict so i can plot it on a precision and recall curve
# im using decision_function since the classification is binary
y_scores_log_reg = cross_val_predict(log_reg_ros, X_train_ros, y_train_ros, cv = 3, method = "decision_function")

precisions_log_reg, recalls_log_reg, thresholds_log_reg = precision_recall_curve(y_train_ros, y_scores_log_reg)

figure(figsize=(8, 6), dpi=80)
plt.plot(recalls_log_reg, precisions_log_reg, linewidth=2, label='Logistic')
plt.xlabel('Recall')
plt.title('Precision Recall Curve Logisitic Regression')
plt.ylabel('Precision')

fig, ax = plt.subplots(figsize=(8, 6))
disp = plot_precision_recall_curve(log_reg_ros, X_test, y_test, name = "Logistic Regression", ax=ax)
disp.ax_.set_title('Precision Recall Curve for Logistic Regression: ')

fpr_log_reg, tpr_log_reg, thresholds_log_reg = roc_curve(y_train_ros, y_scores_log_reg)

fig, ax = plt.subplots(figsize=(8, 6))
metrics.plot_roc_curve(log_reg_ros, X_test, y_test, ax=ax)
plt.plot([0,1],[0,1], 'k--')

# check the auc score
auc_score = roc_auc_score(y_test, y_pred_ros)
print(auc_score)

"""> Everything looks good! I rebalanced tha data multiple times, found the best fit, then trained the data. Then I showed the metrics I should pay attention to when assessing the performance of my model. Finally, the auc score shows that the model performs well!

# Let's try to use decision trees now:
> I'll be using the same rebalanced X_train and y_train, but I'll need to create a new classification algorithm and find/assess a new y_pred.
"""

#X_train, y_train = smote.fit_resample(X_train, y_train)
#class_weight="balanced"

#d_tree = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5, random_state=42),
#                              n_estimators=10,
#                              random_state=0).fit(X_train, y_train)

d_tree = DecisionTreeClassifier(max_depth = 5, random_state = 42)
d_tree.fit(X_train, y_train)
y_pred_dt = d_tree.predict(X_test)

plt.rcParams["figure.figsize"] = (6,6)
unique_elements, counts_elements = np.unique(y_train, return_counts=True)
fig1, ax1 = plt.subplots()

ax1.pie(counts_elements, labels=unique_elements, autopct='%1.1f%%',
        shadow=True, startangle=90, textprops={'fontsize': 18})

plt.show()

#d_tree = DecisionTreeClassifier(max_depth=5, random_state=42, class_weight="balanced").fit(X_train, y_train)
#y_pred_dt = d_tree.predict(X_test)

sns.set_context("poster")

disp = plot_confusion_matrix(d_tree, X_test, y_test,
                    cmap = 'cividis', colorbar=False)

print("Accuracy: ", accuracy_score(y_test, y_pred_dt))
print("Precision: ", precision_score(y_test, y_pred_dt))
print("Recall: ", recall_score(y_test, y_pred_dt))
print("Classifier Score: ", d_tree.score(X_test, y_test))

#X_train, y_train = smote.fit_resample(X_train, y_train)
#class_weight="balanced"

#d_tree = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5, random_state=42),
#                              n_estimators=10,
#                              random_state=0).fit(X_train, y_train)

d_tree_ros = DecisionTreeClassifier(max_depth = 5, random_state = 42)
d_tree_ros.fit(X_train_ros, y_train_ros)
y_pred_dt_ros = d_tree_ros.predict(X_test)

plt.rcParams["figure.figsize"] = (6,6)
unique_elements, counts_elements = np.unique(y_train_ros, return_counts=True)
fig1, ax1 = plt.subplots()

ax1.pie(counts_elements, labels=unique_elements, autopct='%1.1f%%',
        shadow=True, startangle=90, textprops={'fontsize': 18})

plt.show()

#d_tree = DecisionTreeClassifier(max_depth=5, random_state=42, class_weight="balanced").fit(X_train, y_train)
#y_pred_dt = d_tree.predict(X_test)

sns.set_context("poster")

disp = plot_confusion_matrix(d_tree_ros, X_test, y_test,
                    cmap = 'cividis', colorbar=False)

print("Accuracy: ", accuracy_score(y_test, y_pred_dt_ros))
print("Precision: ", precision_score(y_test, y_pred_dt_ros))
print("Recall: ", recall_score(y_test, y_pred_dt_ros))
print("Classifier Score: ", d_tree_ros.score(X_test, y_test))

"""> Using the default dataset without rebalancing works acceptably here. I tried to rebalance and it wasn't worth the tradeoff.

> I will now train the data using the d_tree classifier
"""

# note that i am blowing away the old vals for these vars to keep naming simple
y_train_pred_dt = cross_val_predict(d_tree, X_train, y_train, cv = 3)

y_scores_dt = cross_val_score(d_tree, X_train, y_train, cv = 3)

print("Accuracy: ", accuracy_score(y_train, y_train_pred_dt))
print("Precision: ", precision_score(y_train, y_train_pred_dt))
print("Recall: ", recall_score(y_train, y_train_pred_dt))
print("Classifier Score: ", d_tree.score(X_test, y_test))

"""> Recall actually is pretty low. Let's train it with ROS."""

# note that i am blowing away the old vals for these vars to keep naming simple
y_train_pred_dt_ros = cross_val_predict(d_tree_ros, X_train_ros, y_train_ros, cv = 3)

y_scores_dt_ros = cross_val_score(d_tree_ros, X_train_ros, y_train_ros, cv = 3)

y_train_pred_dt = cross_val_predict(d_tree, X_train, y_train, cv = 3)

y_scores_dt = cross_val_score(d_tree, X_train, y_train, cv = 3)

print("With ROS: ")
print("Accuracy: ", accuracy_score(y_train_ros, y_train_pred_dt_ros))
print("Precision: ", precision_score(y_train_ros, y_train_pred_dt_ros))
print("Recall: ", recall_score(y_train_ros, y_train_pred_dt_ros))
print("Classifier Score: ", d_tree_ros.score(X_test, y_test))

print("Without ROS: ")
print("Accuracy: ", accuracy_score(y_train, y_train_pred_dt))
print("Precision: ", precision_score(y_train, y_train_pred_dt))
print("Recall: ", recall_score(y_train, y_train_pred_dt))
print("Classifier Score: ", d_tree.score(X_test, y_test))

"""> Okay, looks like the results are way better using ROS, so I'll stick with that.

> Decision Trees seem to be preforming worse than logistic regression. I am not able to get the false negatives to lower under 11 without making false positives skyrocket, and that's using a custom weight.
"""

# training again to makes scores usable in graphing against y_train_pred
y_scores_dt_ros = cross_val_predict(d_tree_ros, X_train_ros, y_train_pred_dt_ros, cv = 3)
y_scores_dt = cross_val_predict(d_tree, X_train, y_train_pred_dt, cv = 3)

precisions_dt_ros, recalls_dt_ros, thresholds_dt_ros = precision_recall_curve(y_train_ros, y_scores_dt_ros)
precisions_dt, recalls_dt, thresholds_dt = precision_recall_curve(y_train, y_scores_dt)

figure(figsize=(8, 6), dpi=80)
plt.plot(recalls_dt_ros, precisions_dt_ros, linewidth=2, label='Decision ROS')
plt.plot(recalls_dt, precisions_dt, linewidth=2, label='Decision')
plt.xlabel('Recall')
plt.title('Precision Recall Curve Decision Tree')
plt.ylabel('Precision')
plt.legend()

"""> Precision is so low with ROS that the PRC looks like this. Lets look at the PRC with the original data to compare."""

fig, ax = plt.subplots(figsize=(8, 6))
disp = plot_precision_recall_curve(d_tree_ros, X_test, y_test, name = "Decision Tree ROS", ax=ax)
disp = plot_precision_recall_curve(d_tree, X_test, y_test, name = "Decision Tree", ax=ax)
disp.ax_.set_title('Precision Recall Curve for Decision Tree: ')

fpr_d_tree_ros, tpr_d_tree_ros, thresholds_d_tree_ros = roc_curve(y_train_ros, y_scores_dt_ros)
fpr_d_tree, tpr_d_tree, thresholds_d_tree = roc_curve(y_train, y_scores_dt)

fig, ax = plt.subplots(figsize=(8, 6))
metrics.plot_roc_curve(d_tree, X_test, y_test, ax=ax, label="No Balanacing")
metrics.plot_roc_curve(d_tree_ros, X_test, y_test, ax=ax, label="ROS")
plt.plot([0,1],[0,1], 'k--')

fig, ax = plt.subplots(figsize=(8, 6))
plt.plot(fpr_log_reg, tpr_log_reg, label="Logistic Regression")
plt.plot(fpr_d_tree, tpr_d_tree, label="Decision Tree")
plt.plot(fpr_d_tree_ros, tpr_d_tree_ros, label="Decision Tree w/ ROS")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Logisitc Regression v.s. Decision Tree")

plt.legend(loc=0)

"""> As I said earlier, the algorithm for logistic regression behaved much nicer with balancing algorithms. The best I could do with decisions trees performs much worse compared to logistic regression as shown by the graph above.

# Now, Let's try a Neural Network
"""

def create_model():
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(100, activation = 'relu'))
    model.add(keras.layers.Dense(100, activation = 'relu'))
    model.add(keras.layers.Dense(2, activation = 'sigmoid'))
    model.compile(loss = "sparse_categorical_crossentropy", optimizer = 'adam', metrics = ['accuracy'])
    return model

NN_model = create_model()
model = KerasClassifier(build_fn = create_model, epochs = 5, batch_size = 50, verbose = 0)
history = NN_model.fit(X_train, y_train, epochs = 5)

NN_model_ros = create_model()
model_ros = KerasClassifier(build_fn = create_model, epochs = 5, batch_size = 50, verbose = 0)
history_ros = NN_model.fit(X_train_ros, y_train_ros, epochs = 5)

"""> Looks like once again the ROS set ended up with better results in both loss and accuracy!"""

predict_y = NN_model.predict(X_test) 
classes_y = np.argmax(predict_y,axis=1)

predict_y_ros = NN_model_ros.predict(X_test) 
classes_y_ros = np.argmax(predict_y_ros,axis=1)

from sklearn.metrics import confusion_matrix
NN_cnf = confusion_matrix(classes_y, y_test)
NN_cnf_ros = confusion_matrix(classes_y_ros, y_test)

print("CNF w/o ROS: ")
print(NN_cnf)
print("Accuracy: ", accuracy_score(y_test, classes_y))
print("Precision: ", precision_score(y_test, classes_y))
print("Recall: ", recall_score(y_test, classes_y))

print("CNF w/ ROS: ")
print(NN_cnf_ros)
print("Accuracy: ", accuracy_score(y_test, classes_y_ros))
print("Precision: ", precision_score(y_test, classes_y_ros))
print("Recall: ", recall_score(y_test, classes_y_ros))

NN_report = classification_report(y_test, classes_y)
NN_report_ros = classification_report(y_test, classes_y_ros)
print(NN_report)
print(NN_report_ros)

y_scores_NN = cross_val_predict(model, X_train, y_train, cv = 3)
y_train_pred_NN = cross_val_predict(model, X_train, y_train, cv = 3)

precisions_NN, recalls_NN, thresholds_NN = precision_recall_curve(y_train_pred_NN, y_scores_NN)

y_scores_NN_ros = cross_val_predict(model, X_train_ros, y_train_ros, cv = 3)
y_train_pred_NN_ros = cross_val_predict(model, X_train_ros, y_train_ros, cv = 3)

precisions_NN_ros, recalls_NN_ros, thresholds_NN_ros = precision_recall_curve(y_train_pred_NN_ros, y_scores_NN_ros)

figure(figsize=(8, 6), dpi=80)
plt.plot(recalls_NN_ros, precisions_NN_ros, linewidth=2, label='NN ROS')
plt.plot(recalls_NN, precisions_NN, linewidth=2, label='NN No ROS')
plt.xlabel('Recall')
plt.title('Precision Recall Curve NN')
plt.ylabel('Precision')
plt.legend()

plt.plot(history.history['accuracy'])
plt.plot(history.history['loss'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()