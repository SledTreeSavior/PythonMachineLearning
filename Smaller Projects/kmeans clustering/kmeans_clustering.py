# -*- coding: utf-8 -*-
"""HW5-posted.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/157TcyhycexprOtbtwSlJI41LuNM9E3OU

# COMP4220: Machine Learning, Fall 2021, Assignment 5

# **Due: November 23** 

> ## **Please submit one pdf file for all questions.**

# 1. Describe two techniques to select the right number of clusters when using K-Means.

> ## **Your answer here**

> 1. The Silhouette Score
    Using sklearn silhouette_score you can calculate the best number of clusters. silhouette_score takes the whole dataset and all of its assigned labels. It then finds the silhouette coef where +1 means an instance is inside its own cluster, 0 means an instance is close to a cluster boundary, and -1 means the instance may be in the wrong cluster. Creating a silhouette diagram plots silhouette coefs and can be a more accurate represenation.

> 2. The Elbow
    You can plot the inertia as a function of the number of clusters. There will be a point of inflexion, when the value of splitting into more clusters becomes irrelevant, that is called the elbow. This elbow exists at the best number of clusters provided by the dataset.

# 2. What is the difference between hard clustering and soft clustering?

> ## **Your answer here**

*   Hard clustering assigns each instance to a single cluster
*   Soft clustering gives each instance a score per cluster

# 3. What are some of the main applications of clustering algorithms? (Name at least three applications)

> ## **Your answer here**

* data analysis: running a cluster and analyzing each cluster is good for data analysis
* segmenting images: clustering each pixel by color and changing the color to the mean of these colors (reduces image size)
* search engines: reverse image search relies on a clustered form of all image data on a search engine, so the searched image can be matched with a cluster.

# Programming tasks (KMeans clustering)

## P1.1 Train the Kmeans clustering algorithm on the data with (24, 15), (30, 33), and (54, 24) as the initial cluster centers. Predict the cluster of each data point and show the result. 

<h3> **HINT**
    <ol>
        <li>Initial points should be the 3 givens points</li>
        <li>Use KMeans method for prediction</li>
    </ol>
</h3>
"""

# Importing the libraries
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import pandas as pd

def plot_data(X):
    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=10)

def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):
    if weights is not None:
        centroids = centroids[weights > weights.max() / 10]
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='o', s=35, linewidths=8,
                color=circle_color, zorder=10, alpha=0.9)
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='x', s=20, linewidths=12,
                color=cross_color, zorder=11, alpha=1)

def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,
                             show_xlabels=True, show_ylabels=True):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                         np.linspace(mins[1], maxs[1], resolution))
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                cmap="Pastel2")
    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                linewidths=1, colors='k')
    plot_data(X)
    if show_centroids:
        plot_centroids(clusterer.cluster_centers_)

    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)

from google.colab import drive
drive.mount('/content/drive')
ble_csv = pd.read_csv("/content/drive/My Drive/School/Machine Learning/Homework/HW5/birth_life_expectancies.csv")
print(ble_csv)

X_new = np.array([[24, 15], [30, 33], [54, 24]])
k = 3
kmeans = KMeans(n_clusters=k, random_state=42, init=X_new).fit(X)
print(kmeans.labels_)
print(kmeans.cluster_centers_)
print(kmeans.predict(X))

"""## P1.2 Visualize the clusters with colors"""

plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_.astype(float))

"""# Import the "Mall_Customers" data set"""

dataset = pd.read_csv("/content/drive/My Drive/School/Machine Learning/Homework/HW5/Mall_Customers.csv")
dataset

X = dataset.iloc[:, [2, 4]].values
X

"""# P1.4 Visualize the data set (no clustering)
## xlabel('Age' and ylabel('Spending Score (1-100)')
"""

plt.scatter(X[:, 0], X[:, 1])

"""# P1.5 Fit K-Means to the dataset with n_cluster = 2. 
## Name the kmeans_c2 because we will compare different cluster number to find the optimal number of clusters.
## Set the init of kmeans_c2 to k-means++ for the below tasks

"""

k = 2
kmeans_c2 = KMeans(n_clusters=k, random_state=42).fit(X)
y_pred = kmeans_c2.fit_predict(X)
y_pred is kmeans_c2.labels_

"""# P1.6 Visualize the clusters from P1.5 with colors
<h3> 
    <ol>
        <li>Show different colors for different clusters</li>
        <li>Label red color for the centroids </li>
    </ol>
</h3>
"""

plt.scatter(X[:, 0], X[:, 1], c=kmeans_c2.labels_.astype(float))

"""# P1.7 Does it make sense to use n_cluster = 2? If not then which number of cluster would be appropriate? Give a brief explanation of your answer

> ## Your answer here
> No, it does not make sense. This is because after trying other k values, I was able to get a better silhouette score using 4 clusters instead of 2. This is one way to find an ideal number of clusters (but i didnt go as far as creating a silhouette diagram).

# P1.8 Based on your predict number from your kmeans cluster try to fit K-Means to the dataset with n_cluster = your_answer.
"""

from sklearn.metrics import silhouette_score
score2 = silhouette_score(X, kmeans_c2.labels_)
print(score2)
k = 4
kmeans_c4 = KMeans(n_clusters=k)
y_pred = kmeans_c4.fit_predict(X)
print(y_pred is kmeans_c4.labels_)
score4 = silhouette_score(X, kmeans_c4.labels_)
print(score4)

"""# P1.9 Visualize the clusters in P1.8 with colors
<h3> 
    <ol>
        <li>Show different colors for different clusters</li>
        <li>Label red color for the centroids </li>
    </ol>
</h3>
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

plt.scatter(X[:, 0], X[:, 1], c=kmeans_c4.labels_.astype(float))

"""# P1.10 For a simple data set we might be able to guess the optimal number of clusters. However, if the data set gets more complex we might not be able to guess anymore. Now let's use the elbow method to find the optimal number of clusters through a loop (1 to 10 clusters) and visualize the result on a 2D plot."""

kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)
                for k in range(1, 10)]
inertias = [model.inertia_ for model in kmeans_per_k]
print(kmeans_per_k)
print(inertias)

plt.figure(figsize=(8, 3.5))
plt.plot(range(1, 10), inertias, "bo-")
plt.annotate('Elbow',
             xy=(4, inertias[3]),
             xytext=(0.55, 0.55),
             textcoords='figure fraction',
             fontsize=16,
             arrowprops=dict(facecolor='black', shrink=0.1)
            )
plt.axis([1, 10, 0, 100000])

"""# P1.11 Fit K-Means to the dataset with the optimal n_cluster and show the plot. 

"""

k = 4
kmeans_optimal = KMeans(n_clusters=k)
y_pred = kmeans_optimal.fit_predict(X)
print(y_pred is kmeans_optimal.labels_)
score_optimal = silhouette_score(X, kmeans_optimal.labels_)
print(score_optimal)

plt.scatter(X[:, 0], X[:, 1], c=kmeans_optimal.labels_.astype(float))

plot_decision_boundaries(kmeans_optimal, X)
plt.show()
#decided to make a graph with decisions boundaries, same data as above graph

